<?xml version="1.0"?>
<!--    Plays back raw depth image and RGB image recorded using the associated record.launch from OpenNI device (kinect). The post-processing is then done to get a point-cloud. -->
<launch>

	<!-- Use simulated time: See http://mirror.umd.edu/roswiki/Clock.html for why -->
	<param name="/use_sim_time" value="true"/>

	<!-- Run all the normal processing stuff but tell openni.launch not to actually get any data from a real kinect (that will be supplied by the bag -->
	<include file="$(find openni_launch)/launch/openni.launch" >
		<arg name="load_driver" value="false" />
		<arg name="publish_tf" value="false" />
        <arg name="depth_registration" value="true"/>
        <arg name="camera" value="kinect"/>
	</include>

	<!-- Read back a bag file recorded with the associated record.launch,
	which will then be processed by the nodes launched by openni.launch -->
	<!--<node pkg="rosbag" type="play" name="play" output="screen" required="true" args="$(env PWD)/$(arg BAG_NAME) -delay=1 -clock -l -r 0.5 -s 44"/>-->
	<!--<node pkg="rosbag" type="play" name="play" output="screen" required="true" args="$(find race_learning_by_demonstration)/bags/small_table_3objects.bag -delay=1 -clock -l -r 0.2 -s 62"/>-->
    <node pkg="rosbag" type="play" name="play" output="screen" required="true" args="$(env HOME)/$(arg BAG_NAME) --delay=1 --clock -r 0.1 -s 0 -l -u 5">
    <!--<node pkg="rosbag" type="play" name="play" output="screen" required="true" args="$(env HOME)/$(arg BAG_NAME) -delay=1 -clock -l -r 0.5 -s 0">-->
   <!--<remap from="/camera/depth_registered/camera_info" to="/kinect/depth_registered/camera_info"/>-->
   <!--<remap from="/camera/depth_registered/image_raw" to="/kinect/depth_registered/image_raw"/>-->
   <!--<remap from="/camera/rgb/camera_info" to="/kinect/rgb/camera_info"/>-->
   <!--<remap from="/camera/rgb/image_raw" to="/kinect/rgb/image_raw"/>-->
   </node>
</launch>	
